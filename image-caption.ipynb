{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import pickle\n",
    "from os.path import exists\n",
    "\n",
    "from keras.applications.xception import Xception\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers.merge import add\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "\n",
    "# library to show the progress of loops.\n",
    "from tqdm import tqdm\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare the textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_TEXTS = 'dataset/Flickr8k_text/'\n",
    "PATH_TO_IMAGES = 'dataset/Flickr8k_Dataset/Flicker8k_Dataset/'\n",
    "PATH_TO_TRAIN_FILE_NAMES = PATH_TO_TEXTS + 'Flickr_8k.trainImages.txt'\n",
    "PATH_TO_TEST_FILE_NAMES = PATH_TO_TEXTS + 'Flickr_8k.testImages.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_file(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .\\n1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .\\n1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .\\n1000268201_693b08cb0e.jpg#3\\tA little girl climbing the s'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = get_text_from_file(PATH_TO_TEXTS + 'Flickr8k.token.txt')\n",
    "tokens[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of descriptions in all_descriptions: 40460\n",
      "Expected number of descriptions: 40460\n"
     ]
    }
   ],
   "source": [
    "def get_all_descriptions(text):\n",
    "    descriptions = [line.split('\\t')[-1] for line in text.split('\\n') if len(line) > 1]\n",
    "    return descriptions\n",
    "\n",
    "all_descriptions = get_all_descriptions(tokens)\n",
    "\n",
    "print(f'Total number of descriptions in all_descriptions: {len(all_descriptions)}')\n",
    "print('Expected number of descriptions: ' + str(len(tokens.split(\"\\n\"))-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .',\n",
       " 'A black dog and a spotted dog are fighting',\n",
       " 'A black dog and a tri-colored dog playing with each other on the road .']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_descriptions[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of different file names: 8092\n",
      "Expect number of file names: 8092\n"
     ]
    }
   ],
   "source": [
    "def get_all_file_names(text):\n",
    "    lines = text.split('\\n')\n",
    "    reference = ''\n",
    "    files = []\n",
    "\n",
    "    for line in lines:\n",
    "        file_name = line.split('#')[0]\n",
    "        if reference != (file_name := line.split('#')[0]) and (len(file_name) > 1):\n",
    "            files.append(file_name)\n",
    "            reference = file_name\n",
    "\n",
    "    return files\n",
    "\n",
    "all_file_names = get_all_file_names(tokens)\n",
    "\n",
    "print(f'Number of different file names: {len(all_file_names)}')\n",
    "# There are 5 descriptions per file\n",
    "print(f'Expect number of file names: {int(len(all_descriptions)/5)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin',\n",
       " 'black dog and spotted dog are fighting',\n",
       " 'black dog and tricolored dog playing with each other on the road']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clear_descriptions(descriptions):\n",
    "    clear_descriptions = []\n",
    "    table = str.maketrans('','',string.punctuation)\n",
    "\n",
    "    for description in descriptions:\n",
    "        description = description.replace('-', '').split(' ')\n",
    "        description = [word.translate(table) for word in description]\n",
    "        description = [word.lower() for word in description if (len(word) > 1) and (word.isalpha())]\n",
    "        description = ' '.join(description)\n",
    "        clear_descriptions.append(description)\n",
    "\n",
    "    return clear_descriptions\n",
    "\n",
    "all_descriptions = clear_descriptions(all_descriptions)\n",
    "all_descriptions[0:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(all_descriptions): 40460\n"
     ]
    }
   ],
   "source": [
    "print(f'len(all_descriptions): {len(all_descriptions)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(image_and_descriptions_dict): 8092\n",
      "len(all_file_names): 8092\n"
     ]
    }
   ],
   "source": [
    "def get_image_and_descriptions_dictionary(file_names, all_descriptions):\n",
    "    image_and_descriptions_dict = {}\n",
    "\n",
    "    for idx, file in enumerate(file_names):\n",
    "        descriptions = []\n",
    "        for i in range(5):\n",
    "            descriptions.append(all_descriptions[i + idx*5])\n",
    "        \n",
    "        image_and_descriptions_dict[file] = descriptions\n",
    "    \n",
    "    return image_and_descriptions_dict\n",
    "\n",
    "image_and_descriptions_dict = get_image_and_descriptions_dictionary(all_file_names, all_descriptions)\n",
    "\n",
    "print(f'len(image_and_descriptions_dict): {len(image_and_descriptions_dict)}')\n",
    "print(f'len(all_file_names): {len(all_file_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000268201_693b08cb0e.jpg\n",
      "child in pink dress is climbing up set of stairs in an entry way\n",
      "girl going into wooden building\n",
      "little girl climbing into wooden playhouse\n",
      "little girl climbing the stairs to her playhouse\n",
      "little girl in pink dress going into wooden cabin\n"
     ]
    }
   ],
   "source": [
    "print(all_file_names[0])\n",
    "for i in range(5):\n",
    "    print(all_descriptions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['child in pink dress is climbing up set of stairs in an entry way',\n",
       " 'girl going into wooden building',\n",
       " 'little girl climbing into wooden playhouse',\n",
       " 'little girl climbing the stairs to her playhouse',\n",
       " 'little girl in pink dress going into wooden cabin']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_and_descriptions_dict['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997722733_0cb5439472.jpg\n",
      "man in pink shirt climbs rock face\n",
      "man is rock climbing high in the air\n",
      "person in red shirt climbing up rock face covered in assist handles\n",
      "rock climber in red shirt\n",
      "rock climber practices on rock climbing wall\n"
     ]
    }
   ],
   "source": [
    "print(all_file_names[-1])\n",
    "for i in range(-5, 0):\n",
    "    print(all_descriptions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['man in pink shirt climbs rock face',\n",
       " 'man is rock climbing high in the air',\n",
       " 'person in red shirt climbing up rock face covered in assist handles',\n",
       " 'rock climber in red shirt',\n",
       " 'rock climber practices on rock climbing wall']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_and_descriptions_dict['997722733_0cb5439472.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for inconsistencies in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_missing_files(files_dictionary):\n",
    "    missing_files = []\n",
    "    cnt = 0\n",
    "\n",
    "    for file in files_dictionary:\n",
    "        if not exists(PATH_TO_IMAGES + file):\n",
    "            missing_files.append(file)\n",
    "            cnt += 1\n",
    "            print(f'File not found: {file}')\n",
    "    if cnt == 0:\n",
    "        print('There are no missing files')\n",
    "        return None\n",
    "    else:\n",
    "        print(f'Total missing files: {cnt}')\n",
    "        return missing_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: 2258277193_586949ec62.jpg.1\n",
      "Total missing files: 1\n"
     ]
    }
   ],
   "source": [
    "missing_files = check_missing_files(image_and_descriptions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['people waiting for the subway',\n",
       " 'some people looking out windows in large building',\n",
       " 'three people are waiting on train platform',\n",
       " 'three people standing at station',\n",
       " 'two woman and one man standing near train tracks']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_and_descriptions_dict['2258277193_586949ec62.jpg.1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2258277193_586949ec62.jpg.1']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the dictionary entry of the missing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in missing_files:\n",
    "    image_and_descriptions_dict.pop(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no missing files\n"
     ]
    }
   ],
   "source": [
    "check_missing_files(image_and_descriptions_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary entry removed!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    image_and_descriptions_dict['2258277193_586949ec62.jpg.1']\n",
    "except:\n",
    "    print('Dictionary entry removed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of of all_file_names before removing the missing file: 8092\n",
      "Lenght of of all_file_names after removing the missing file: 8091\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght of of all_file_names before removing the missing file: {len(all_file_names)}')\n",
    "all_file_names = [key for key in image_and_descriptions_dict]\n",
    "print(f'Lenght of of all_file_names after removing the missing file: {len(all_file_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght of of all_descriptions before removing the missing file: 40460\n",
      "Lenght of of all_descriptions after removing the missing file: 40455\n"
     ]
    }
   ],
   "source": [
    "print(f'Lenght of of all_descriptions before removing the missing file: {len(all_descriptions)}')\n",
    "all_descriptions = []\n",
    "for key in image_and_descriptions_dict:\n",
    "    for description in image_and_descriptions_dict[key]:\n",
    "        all_descriptions.append(description)\n",
    "print(f'Lenght of of all_descriptions after removing the missing file: {len(all_descriptions)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vocabulary): 8763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['persian',\n",
       " 'floors',\n",
       " 'needs',\n",
       " 'countryside',\n",
       " 'character',\n",
       " 'balloons',\n",
       " 'mingle',\n",
       " 'garland',\n",
       " 'waterskis',\n",
       " 'partly']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vocabulary(all_descriptions):\n",
    "    vocabulary = set()\n",
    "    for description in all_descriptions:\n",
    "        for word in description.split():\n",
    "            if len(word) > 1:\n",
    "                vocabulary.add(word)\n",
    "    return list(vocabulary)\n",
    "\n",
    "vocabulary = get_vocabulary(all_descriptions)\n",
    "print(f'len(vocabulary): {len(vocabulary)}')\n",
    "vocabulary[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(files_test_raw): 1000\n",
      "len(files_test): 1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['3385593926_d3e9c21170.jpg',\n",
       " '2677656448_6b7e7702af.jpg',\n",
       " '311146855_0b65fdb169.jpg']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_test_raw = get_all_file_names(get_text_from_file(PATH_TO_TEST_FILE_NAMES))\n",
    "files_test = [file for file in files_test_raw if file not in missing_files]\n",
    "print(f'len(files_test_raw): {len(files_test_raw)}')\n",
    "print(f'len(files_test): {len(files_test)}')\n",
    "files_test[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(files_train_raw): 6000\n",
      "len(files_train): 6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['2513260012_03d33305cf.jpg',\n",
       " '2903617548_d3e38d7f88.jpg',\n",
       " '3338291921_fe7ae0c8f8.jpg']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_train_raw = get_all_file_names(get_text_from_file(PATH_TO_TRAIN_FILE_NAMES))\n",
    "files_train = [file for file in files_train_raw if file not in missing_files]\n",
    "print(f'len(files_train_raw): {len(files_train_raw)}')\n",
    "print(f'len(files_train): {len(files_train)}')\n",
    "files_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features using Xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xception_features(img_files):\n",
    "        model = Xception(include_top=False, pooling='avg')\n",
    "        features = {}\n",
    "        \n",
    "        for img in tqdm(img_files):\n",
    "            file_path = PATH_TO_IMAGES + img\n",
    "            image = Image.open(file_path)\n",
    "        \n",
    "            # Adjust the image to fit in the pre-trained model\n",
    "            image = image.resize((299,299))\n",
    "            image = np.expand_dims(image, axis=0)\n",
    "            image = image/127.5\n",
    "            image = image - 1.0\n",
    "        \n",
    "            feature = model.predict(image)\n",
    "            features[img] = feature\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont run the cell below. It takes 40 minutes to run.\n",
    "\n",
    "\n",
    "Instead, use the features.pkl object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8091/8091 [40:41<00:00,  3.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# features = xception_features(all_file_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save/Load the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('features.pkl', 'wb') as f:\n",
    "#     pickle.dump(features, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('features.pkl', 'rb') as f:\n",
    "    features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {key: image_and_descriptions_dict[key] for key in files_train}\n",
    "all_train_descriptions = []\n",
    "for key in train_dict:\n",
    "    key_descriptions = []\n",
    "    for description in train_dict[key]:\n",
    "        description = 'start_description ' + description + ' end_description'\n",
    "        all_train_descriptions.append(description)\n",
    "        key_descriptions.append(description)\n",
    "    train_dict[key] = key_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'start_description black dog is running after white dog in the snow end_description'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_train_descriptions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = {key: features[key] for key in files_train}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_train_descriptions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the maximum length of the descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary_lenght: 7578\n"
     ]
    }
   ],
   "source": [
    "vocabulary_lenght = len(tokenizer.word_index) + 1\n",
    "print(f'vocabulary_lenght: {vocabulary_lenght}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "descriptions_max_length: 32\n"
     ]
    }
   ],
   "source": [
    "descriptions_max_length = 0\n",
    "for description in all_descriptions:\n",
    "    if len(description.split()) > descriptions_max_length:\n",
    "        descriptions_max_length = len(description.split())\n",
    "\n",
    "print(f\"descriptions_max_length: {descriptions_max_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_descriptions = [image_and_descriptions_dict[key] for key in files_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the dogs are in the snow in front of fence',\n",
       "  'the dogs play on the snow',\n",
       "  'two brown dogs playfully fight in the snow',\n",
       "  'two brown dogs wrestle in the snow',\n",
       "  'two dogs playing in the snow'],\n",
       " ['brown and white dog swimming towards some in the pool',\n",
       "  'dog in swimming pool swims toward sombody we cannot see',\n",
       "  'dog swims in pool near person',\n",
       "  'small dog is paddling through the water in pool',\n",
       "  'the small brown and white dog is in the pool'],\n",
       " ['man and woman in festive costumes dancing',\n",
       "  'man and woman with feathers on her head dance',\n",
       "  'man and woman wearing decorative costumes and dancing in crowd of onlookers',\n",
       "  'one performer wearing feathered headdress dancing with another performer in the streets',\n",
       "  'two people are dancing with drums on the right and crowd behind them']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_descriptions[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(image_and_descriptions_dict, features, tokenizer, descriptions_max_length):\n",
    "    while True:\n",
    "        for image, descriptions in image_and_descriptions_dict.items():\n",
    "            feature = features[image][0]\n",
    "            input_image, input_sequence, output_word = create_sequences(tokenizer, descriptions_max_length, descriptions, feature)\n",
    "            \n",
    "            yield [[input_image, input_sequence], output_word]         \n",
    "\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocabulary_lenght)[0]\n",
    "            # store\n",
    "            X1.append(feature)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    \n",
    "    return np.array(X1), np.array(X2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((57, 2048), (57, 32), (57, 7578))"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a,b],c = next(data_generator(train_dict, features, tokenizer, descriptions_max_length))\n",
    "a.shape, b.shape, c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features from the CNN model squeezed from 2048 to 256 nodes\n",
    "inputs1 = Input(shape=(2048,))\n",
    "fe1 = Dropout(0.5)(inputs1)\n",
    "fe2 = Dense(256, activation='relu')(fe1)\n",
    "\n",
    "# LSTM sequence model\n",
    "inputs2 = Input(shape=(descriptions_max_length,))\n",
    "se1 = Embedding(vocabulary_lenght, 256, mask_zero=True)(inputs2)\n",
    "se2 = Dropout(0.5)(se1)\n",
    "se3 = LSTM(256)(se2)\n",
    "\n",
    "# Merging both models\n",
    "decoder1 = add([fe2, se3])\n",
    "decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "outputs = Dense(vocabulary_lenght, activation='softmax')(decoder2)\n",
    "\n",
    "# tie it together [image, seq] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('models/my_model04.01_24.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "steps = len(train_dict)\n",
    "\n",
    "for i in range(epochs):\n",
    "    generator = data_generator(train_dict, train_features, tokenizer, descriptions_max_length)\n",
    "    model.fit(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "    model.save(\"models/my_model04.01_\" + str(i+25) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(filename, model):\n",
    "        try:\n",
    "            image = Image.open(filename)\n",
    "\n",
    "        except:\n",
    "            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n",
    "        image = image.resize((299,299))\n",
    "        image = np.array(image)\n",
    "        # for images that has 4 channels, we convert them into 3 channels\n",
    "        if image.shape[2] == 4: \n",
    "            image = image[..., :3]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        image = image/127.5\n",
    "        image = image - 1.0\n",
    "        feature = model.predict(image)\n",
    "        return feature\n",
    "\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    in_text = 'start'\n",
    "    for i in range(max_length):\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        pred = model.predict([photo,sequence], verbose=0)\n",
    "        pred = np.argmax(pred)\n",
    "        word = word_for_id(pred, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end':\n",
    "            break\n",
    "    return in_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(file):\n",
    "    img_path = PATH_TO_IMAGES + file\n",
    "    # model = load_model('models/my_model_9.h5')\n",
    "    xception_model = Xception(include_top=False, pooling=\"avg\")\n",
    "\n",
    "    photo = extract_features(img_path, xception_model)\n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    description = generate_desc(model, tokenizer, photo, descriptions_max_length)\n",
    "    print(\"\\n\\n\")\n",
    "    print(description[18:-3])\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('3385593926_d3e9c21170.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('2677656448_6b7e7702af.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('311146855_0b65fdb169.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('1258913059_07c613f7ff.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('241347760_d44c8d3a01.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('2654514044_a70a6e2c21.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('2339106348_2df90aa6a9.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('256085101_2c2617c5d0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('280706862_14c30d734a.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prediction('3072172967_630e9c69d0.jpg')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26ab4e3eda37a59b3f170175b0524bf8ba674c8b1a4bbefa920a0a9c79b3fd73"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('aml_3104-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
